\section{Related work}
\label{sec:related_work}


\paragraph{Supervised direct learning of SNNs.}
Based on the idea that SNNs could be optimized end-to-end through back-propagation \cite{rumelhart1986learning}, Bothe \etal \cite{bohte2000spikeprop} first used the surrogate gradient (SG) to solve the non-differentiability of SNNs. Wu \etal \cite{wu2018spatio} further compared and analyzed the impacts of the shapes of SG functions on model performance. Afterwards, many practical techniques and architectural designs were developed to strengthen the direct-trained SNN performance, such as tdBN \cite{zheng2021going}, SEW block \cite{fang2021deep}, spiking self-attention (SSA) block \cite{zhou2022spikformer}, TET \cite{deng2021temporal}, \etc. In this paper, we adopt the two main-stream direct-trained SEW and SSA to estimate the effectiveness of our proposed methods.
\\\textbf{Multi-bit spiking neural networks.}
Wu \etal \cite{wu2021liaf} were the first to replace binary spike by full precision analog spike to improve the model accuracy. Guo \etal further added the spike amplitude coefficient \cite{guo2022real} and negative spike \cite{guo2024ternary} to realize ternary (2-bit) spike. You \etal \cite{xing2024spikelm} and Xing \etal \cite{xing2024spikelm} concurrently introduced ternary spike into natural language processing (NLP) and both achieved huge success. SpikeLLM \cite{xing2024spikellm} and Multi-bit SNN \cite{xiao2024multi} were more aggressive. They extended ternary spike to multiple bits (over 2 bits) to boost the SNN performance while theoretically proved that the characteristic of addition-only computation can still be maintained. Shen \etal \cite{shen2024conventional} combined multi-bit SNNs with quantization and presented a rational and practical approach called “Bit Budget” to estimating the computation overhead. Different from the above multi-bit SNNs, our work focus on fine-grained bit allocations, with which multi-bit SNNs can substantially reduce the computation overhead and squeeze the model accuracy to its limit.
\\\textbf{Mixed-precision quantization.}
Mixed-precision quantization is based on the insight that different layers in artificial neural networks (ANN) require different degrees of representation ability. Wang \etal \cite{wang2019haq} and Wu \etal \cite{wu2018mixed} were the pioneers that fostered the search-based means to allocate  different bit widths with reinforcement learning and differentiable neural architecture search, respectively. While, Dong \etal \cite{dong2019hawq} used Hessian metrics to represent the layer importance and accordingly assigned bit widths. Recently, optimization-based mixed-precision was developed to learn the bit width during training \cite{uhlich2019mixed,zhang2021differentiable,huang2022sdq,kim2024metamix}.
Compared with the above methods that solely focus on ANN mixed-precision, our work, that features different gradient calculations, tackling the extra time dimension, and alleviating the step-size mismatch issue, is the first to target at SNNs. 

