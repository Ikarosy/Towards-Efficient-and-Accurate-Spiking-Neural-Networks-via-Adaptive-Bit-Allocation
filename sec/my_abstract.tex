\begin{abstract}
Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, persuing energy-efficient and high-accurate AI. 
However, with more bits involved, the memory and computation requirements grow so significantly that the substantial performance lift become out of proportion. 
Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation method for direct-trained SNNs, advancing efficiency and accuracy. 
Specifically, we parametrize the bit widths of weights, spikes and temporal lengths, and make them learnable and controllable through gradients. 
Tackling rising issues  due to changeable bits and temporal lengths, we propose 
the refined spiking neuron that can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suits spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of  learnable widths, which may incur severe inference errors to SNN, and accordingly propose the step-size renew mechanism to alleviate this issue. 
Extensive experiments on a variety of datasets, including the static CIFAR and ImageNet and the dynamic CIFAR-DVS, demonstrate that our method can reduce the memory and computation cost by a significant margin, while achieving higher classification accuracy. Particularly, our SEWResNet-34 with 15.4 bit budgets can achieve the advanced 72.82\% top-1 accuracy on ImageNet.
This project will be open-sourced.


\end{abstract}